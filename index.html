<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8" />
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@200;300;400;600;700;900&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="assets/css/style.css" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-DPH2YST3Z6"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-DPH2YST3Z6');
    </script>

    <title> Heng (Alfredo) Zhang </title>
    <!-- <link rel="icon" type="image/x-icon" href="assets/images/iit"> -->
</head>

<body>
    <div style="width:1000px;margin: 0px auto;">
        <header id="header" width="400px" style="display:flex; justify-content: space-around; border-radius: 20px; padding: 10px; background: #076bce; color: rgb(145, 19, 19);">
            <a href="#profile-intro" style="color: white; font-weight: bold;">Home</a>
            <a href="#updates" style="color: white; font-weight: bold;">News</a>
            <a href="#research" style="color: white; font-weight: bold;">Research</a>
            <a href="#service" style="color: white; font-weight: bold;">Service</a>
            <a href="misc.html" style="color: white; font-weight: bold;">Misc</a>
        </header>
        <div id="profile">
            <div id="profile-pic">
                <img src="assets/images/profile-image.jpg" style="width:250px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);" />
            </div>
            <div id="profile-intro">
                <div id="profile-name">Heng (Alfredo) Zhang</div>
                <div id="profile-email">heng.zhang@iit.it</div>
                <p>
                    <div style="text-align: justify;">
                    I am a third-year PhD student in Robotics and Intelligent Machines (DRIM) at  Italian Institute of Technology (IIT), advised by Dr. <a href="https://www.iit.it/it/people-details/-/people/arash-ajoudani">Arash Ajoudani</a>.
                    Previously, I received a Master's degree in Control Engineering from Tongji University and a B.E degree in Automation from Northeast Electric Power University, China. 
                    <br>
                    </div>
                </p>
                <div style="text-align: center;">
                    <a href="assets/documents/Heng_Zhang_CV.pdf" target="_blank" id="cv">CV</a>
                    /
                    <a href="https://scholar.google.com.hk/citations?hl=en&pli=1&user=5KhgNnoAAAAJ" id="gscholar">Google Scholar</a>                      
                    /
                    <a href="https://www.linkedin.com/in/heng-zhang-ab3699219/">LinkedIn</a>
                    /
                    <a href="https://www.researchgate.net/profile/Heng-Zhang-191?ev=hdr_xprf">ResearchGate</a>
                    /
                    <a href="https://x.com/HengZha70061681">Twitter</a>
                    /
                    <a href="https://github.com/jack-sherman01">Github</a>
                </div>
            </div>
                        
            <div style="clear: both;"></div>
        </div>

        <div class="divider"></div>

        <div style="text-align: justify;">
        <!-- Update Research Interests -->
        <h1 style="font-size: 2em; color: #076bce; text-shadow: 1px 1px 2px rgba(0,0,0,0.1);">Research Interests</h1>

            <blockquote>
              <p>"Contact is the heart of robotic manipulation. To understand manipulation, you must understand contact."</p>
                  <footer>â€” Matthew T. Mason, <a href="https://mtmason.com/the-heart-of-robotic-manipulation/">The Heart of Robotic Manipulation</a></footer>
            </blockquote>
        My research focuses on robot learning, reinforcement learning. Specifically, My current research interest focuses on:
        <ul>
        <li> Â· <b>Learning-based methods for contact-rich robotic tasks</b>, with a focus on safety issues including safe exploration and execution;</li>
        <li> Â· <b>RL fine-tuning for safe VLA-driven contact manipulation</b> (multimodal VLA including force-torque, vision, tactile, and language);</li>
        <li> Â· <b>AI4Sci</b>.</li>
        </ul>
        I would be happy you want to discuss my research with me. <strong>Iâ€™m open to collaborations, please feel free to send me Email!</strong>
        </div>
        <br>
        <div style="clear: both;"></div>
        <div class="divider"></div>
        <section id="career-goals">
            <h2>Career Goals</h2>
            <div style="display: flex; gap: 20px;">
                <div style="flex: 1;">
                    <h3>Short-term:</h3>
                    <ul>
                        <li>Complete PhD</li>
                        <li>Apply for a postdoc</li>
                    </ul>
                </div>
                <div style="flex: 1;">
                    <h3>Long-term:</h3>
                    <ul>
                        <li>To be a professor or:</li>
                        <li>Co-found a robotics startup</li>
                    </ul>
                </div>
            </div>
        </section>
        <br>
        <div style="clear: both;"></div>
        <div class="divider"></div> <div class="divider"></div>

        <div class="section" id="Outreach">
            <!-- Update Outreach -->
            <h1 style="font-size: 1.8em; color: #0066cc; font-weight: 700;">Outreach</h1>
            <p>Inspired by <a href="https://shuijing725.github.io/">Shuijing Liu</a>: For junior PhD, Master's, and undergraduate students as well as potential collaborators, I offer a 30-minute mentorship session. I am especially available to support students from underrepresented groups or those in need. Topics include, but are not limited to, AI, robotics, AI4Sci research, graduate school applications, career development, and life advice. If you'd like to chat, please <a href="https://docs.google.com/forms/d/e/1FAIpQLSdhu9ZzP62rHibKC-VggM01m3KVXRkdxzWzwY3HuXo407HgnA/viewform?usp=sharing" target="_blank">fill out this form</a> to schedule a meeting.</p>
            <p><strong>Note:</strong> I do check my email every weekday and respond promptly. Please feel free to send a follow-up email if you haven't received a reply.</p>
            <br>
            <div style="clear: both;"></div>
        </div>
        <div class="divider"></div> <div class="divider"></div>
        
        <div class="section" id="updates">
            <!-- Update News -->
            <h1 style="font-size: 1.8em; color: #0a8aff; font-weight: 700;">News</h1>
            <ul>
                <li> <b>Apr 2025</b> this website was born. 
                <li> <b>Jul 2023</b> Started summer school at ETHz in learning-based control. 
                <li> <b>Nov 2022</b> Started PhD at iit in Robotics and Intelligent Machines (DRIM). 
                <li> <b>Jun 2022</b> Graduated from Tongji University with Master degree as an outstanding graduate ðŸŽ“.
            </ul>
            <p></p>
            <div style="clear: both;"></div>
        </div>
        <div class="divider"></div>
        <div style="display:flex;flex-direction: column;" id="research">
            <!-- Update Research -->
            <h1 style="font-size: 1.8em; color: #076bce; font-weight: 700;">Research</h1>
            <br>
            <br>
            <!-- SRL-VIC paper -->
            <div class="research-card">
                <div class="research-image">
                    <img src="assets/images/SRL_VIC_Y.gif" alt="SRL-VIC Animation">
                </div>
                <div class="research-content">
                    <div class="research-title">
                        SRL-VIC: A variable stiffness-based safe reinforcement learning for contact-rich robotic tasks
                    </div>
                    <div class="research-authors">
                        <b>Heng Zhang</b>, 
                        <a href="https://scholar.google.co.uk/citations?user=yuOV2TQAAAAJ&hl=tr">Gokhan Solak</a>, 
                        <a href="https://scholar.google.com.br/citations?user=dmGqrDYAAAAJ&hl=en/">Gustavo JG Lahr</a>,
                        <a href="https://www.iit.it/it/people-details/-/people/arash-ajoudani">Arash Ajoudani</a>
                    </div>
                    <div class="research-venue">
                        IEEE Robotics and Automation Letters (RA-L), 2024
                    </div>
                    <div class="research-links">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10517611"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://www.youtube.com/watch?v=ksWXR3vByoQ"><i class="fas fa-video"></i> Video</a>
                    </div>
                    <div class="research-abstract">
                        Exploration Policy with Safety and Generalization in contact-rich tasks using Safe Reinforcement Learning and VIC.
                    </div>
                </div>
            </div>

            <!-- INTENTION paper -->
            <div class="research-card">
                <div class="research-image">
                    <img src="assets/images/intention.png" alt="INTENTION Animation">
                </div>
                <div class="research-content">
                    <div class="research-title">
                        INTENTION: Inferring Tendencies of Humanoid Motion Through Physical Intuition and Grounded VLM
                    </div>
                    <div class="research-authors">
                        Jin Wang, Weijie Wang, Boyuan Deng, <b>Heng Zhang</b>, Rui Dai, Nikos Tsagarakis
                    </div>
                    <div class="research-venue">
                        IEEE-RAS International Conference on Humanoid Robots, Seoul, Korea, 2025
                    </div>
                    <div class="research-links">
                        <a href="https://arxiv.org/abs/2508.04931/"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://robo-intention.github.io/"><i class="fas fa-globe"></i> Website</a>
                    </div>
                    <div class="research-abstract">
                        INTENTION is a framework that combines physical intuition and grounded VLM to infer humanoid motion tendencies, enabling robots to predict and adapt to human actions in dynamic environments.
                    </div>
                </div>
            </div>

            <!-- ActivePose paper -->
            <div class="research-card">
                <div class="research-image">
                    <img src="assets/images/activePose.jpg" alt="ActivePose Animation">
                </div>
                <div class="research-content">
                    <div class="research-title">
                        ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation
                    </div>
                    <div class="research-authors">
                        Sheng Liu, Zhe Li, Weiheng Wang, Han Sun, <b>Heng Zhang</b>, Hongpeng Chen, Yusen Qin, Arash Ajoudani, Yizhao Wang
                    </div>
                    <div class="research-venue">
                        IEEE The International Conference on Robotics and Automation (ICRA), 2026, under review
                    </div>
                    <div class="research-links">
                        <a href="https://arxiv.org/abs/2509.11364"><i class="fas fa-file-pdf"></i> Paper</a>
                    </div>
                </div>
            </div>

            <!-- aiXiv paper -->
            <div class="research-card">
                <div class="research-image">
                    <img src="assets/images/aixiv_sys.jpeg" alt="aiXiv Animation">
                </div>
                <div class="research-content">
                    <div class="research-title">
                        aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists
                    </div>
                    <div class="research-authors">
                        <a href="https://universea.github.io/">Pengsong Zhang</a>, <b>Heng Zhang</b>, et al.
                    </div>
                    <div class="research-venue">
                        The 40th Annual AAAI Conference on Artificial Intelligence, 2026, under review
                    </div>
                    <div class="research-links">
                        <a href="https://arxiv.org/abs/"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://github.com/aixiv-org/aiXiv"><i class="fab fa-github"></i> Code</a>
                        <a href="https://www.aixiv.co/"><i class="fas fa-globe"></i> Website</a>
                    </div>
                    <div class="research-abstract">
                        aiXiv is a Preprint server for AI Scientists and Robot Scientists that leverages AI technologies to facilitate scientific discovery and collaboration among researchers.
                    </div>
                </div>
            </div>

            <!-- HiBerNAC paper -->
            <div class="research-card">
                <div class="research-image">
                    <img src="assets/images/HiBerNAC.png" alt="HiBerNAC Animation">
                </div>
                <div class="research-content">
                    <div class="research-title">
                        HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation
                    </div>
                    <div class="research-authors">
                        Hongjun Wu, <b>Heng Zhang</b>,
                        <a href="https://universea.github.io/">Pengsong Zhang</a>,
                        Jin Wang, Cong Wang
                    </div>
                    <div class="research-venue">
                        under review
                    </div>
                    <div class="research-links">
                        <a href="https://arxiv.org/abs/2506.08296"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://anonymous.4open.science/r/HiBerNAC-0E2F"><i class="fas fa-code"></i> Code</a>
                    </div>
                    <div class="research-abstract">
                        HiBerNAC: a Hierarchical Brain-emulated robotic Neural Agent Collective that combines: (1) multimodal VLA planning and reasoning with (2) neuro-inspired reflection and multi-agent mechanisms, specifically designed for complex robotic manipulation tasks.
                    </div>
                </div>
            </div>

            <!-- Passive Safe RL paper -->
            <div class="research-card">
                <div class="research-image">
                    <img src="assets/images/passiveRL_right_corner3.gif" alt="passiveRL Animation">
                </div>
                <div class="research-content">
                    <div class="research-title">
                        Towards Passive Safe Reinforcement Learning: A Comparative Study on Contact-rich Robotic Manipulation
                    </div>
                    <div class="research-authors">
                        <b>Heng Zhang</b>,
                        <a href="https://scholar.google.co.uk/citations?user=yuOV2TQAAAAJ&hl=tr">Gokhan Solak</a>,
                        <a href="https://scholar.google.com/citations?user=8kn7Z9QAAAAJ&hl=en&oi=ao">Sebastian Hjorth</a>,
                        <a href="https://www.iit.it/it/people-details/-/people/arash-ajoudani">Arash Ajoudani</a>
                    </div>
                    <div class="research-venue">
                        IEEE Robotics and Automation Letters (RA-L) under review
                    </div>
                    <div class="research-links">
                        <a href="https://arxiv.org/abs/2503.00287"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://huggingface.co/papers/2503.00287"><i class="fas fa-brain"></i> Hugging Face</a>
                    </div>
                    <div class="research-abstract">
                        Learning to be safe and stable both in training and deployment in real world.
                    </div>
                </div>
            </div>

            <!-- Bresa paper -->
            <div class="research-card">
                <div class="research-image">
                    <img src="assets/images/Bresa_framework.jpg" alt="Bresa">
                </div>
                <div class="research-content">
                    <div class="research-title">
                        Bresa: Bio-inspired Reflexive Safe Reinforcement Learning for Contact-Rich Robotic Tasks
                    </div>
                    <div class="research-authors">
                        <b>Heng Zhang*</b>,
                        <a href="https://scholar.google.co.uk/citations?user=yuOV2TQAAAAJ&hl=tr">Gokhan Solak*</a>,
                        <a href="https://www.iit.it/it/people-details/-/people/arash-ajoudani">Arash Ajoudani</a>
                        * equal contribution
                    </div>
                    <div class="research-venue">
                        IEEE Robotics and Automation Letters (RA-L) under review
                    </div>
                    <div class="research-links">
                        <a href="https://arxiv.org/abs/2503.21989"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://jack-sherman01.github.io/Bresa"><i class="fas fa-globe"></i> Website</a>
                        <a href="https://youtu.be/fWKhCDTcqzs"><i class="fas fa-video"></i> Video</a>
                    </div>
                    <div class="research-abstract">
                        A Bio-inspired Reflexive Hierarchical Safe RL method inspired by biological reflexes operating at a higher frequency than the task solver.
                    </div>
                </div>
            </div>

            <!-- AGS paper -->
            <div class="research-card">
                <div class="research-image">
                    <img src="assets/images/ags_brain.jpg" alt="AGS">
                </div>
                <div class="research-content">
                    <div class="research-title">
                        Scaling Laws in Scientific Discovery with AI and Robot Scientists
                    </div>
                    <div class="research-authors">
                        <a href="https://universea.github.io/">Pengsong Zhang*</a>, <b>Heng Zhang*</b>,
                        <a href="http://hxu.rocks/publication.html/">Huazhe Xu</a>, Renjun Xu,
                        <a href="https://zhentingwang.github.io/">Zhenting Wang</a>, Cong Wang,
                        <a href="https://animesh.garg.tech/">Animesh Garg</a>, Zhibin Li,
                        <a href="https://www.iit.it/it/people-details/-/people/arash-ajoudani">Arash Ajoudani</a>,
                        <a href="https://www.mie.utoronto.ca/faculty_staff/xinyu-liu/">Xinyu Liu</a>
                        * equal contribution
                    </div>
                    <div class="research-venue">
                        Nature Machine Intelligence in submission
                    </div>
                    <div class="research-links">
                        <a href="https://arxiv.org/abs/2503.22444"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://huggingface.co/papers/2503.22444"><i class="fas fa-brain"></i> Hugging Face</a>
                        <a href="https://github.com/openags/Awesome-AI-Scientist-Papers"><i class="fab fa-github"></i> Github</a>
                    </div>
                    <div class="research-abstract">
                        Autonomous Generalist Scientist (AGS) combines agentic AI and embodied robotics to automate the entire research lifecycle.
                    </div>
                </div>
            </div>

            <!-- SVSLAM Survey paper -->
            <div class="research-card">
                <div class="research-image">
                    <img src="assets/images/vslam_survey.jpg" alt="SVSLAM Survey">
                </div>
                <div class="research-content">
                    <div class="research-title">
                        Semantic visual simultaneous localization and mapping: A survey
                    </div>
                    <div class="research-authors">
                        Kaiqi Chen, Junhao Xiao, Jialing Liu, Qiyi Tong, <b>Heng Zhang</b>, Ruyu Liu, Jianhua Zhang, Arash Ajoudani, Shengyong Chen
                    </div>
                    <div class="research-venue">
                        IEEE Transactions on Intelligent Transportation Systems, 2025
                    </div>
                    <div class="research-links">
                        <a href="https://ieeexplore.ieee.org/abstract/document/11005698"><i class="fas fa-file-pdf"></i> Paper</a>
                    </div>
                    <div class="research-abstract">
                        Semantic visual simultaneous localization and mapping (SVSLAM) is a crucial task in robotics and computer vision, aiming to simultaneously estimate the robot's location and map the environment using semantic information.
                    </div>
                </div>
            </div>
        </div>
        <div class="divider"></div>
        <div class="section" id="service">
            <!-- Update Service -->
            <h1 style="font-size: 1.8em; color: #0052a3; font-weight: 700;">Service</h1>
            <ul>
                <li> Co-organizer of <a href="https://jack-sherman01.github.io/AIRobot4Sci/">IROS 2025 1st Workshop on Embodied AI and Robotics for Future Scientific Discovery</a></li>
                <li> Reviewer of IEEE TCDS, RA-L, AAAI, NIPS, IROS, RO-MAN, CAC, etc.
            </ul>
            <p></p>
            <div style="clear: both;"></div>
        </div>
    </div>
    <!-- inster a map -->
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=zIt01Mzu4TTf_bTcyIsPp4p8X9AplOp1Bk5BA84aMJc"></script>
    <!-- Add this code where you want the counter to appear -->
    <!-- <div align=center><a href='https://www.counter12.com'><img src='https://www.counter12.com/img-49BcxC1Zy9yZbD1C-36.gif' border='0' alt='conter12'></a><script type='text/javascript' src='https://www.counter12.com/ad.js?id=49BcxC1Zy9yZbD1C'></script></div> -->

    <footer style="text-align: center;">
        <div>
            <p class="privacy-notice" style="text-align: center; font-size: 12px;">
                This map shows approximate locations of visitors. No personally identifiable information is stored.
            </p>
            <span style="font-size: 12px;">
                Updated from <a href="https://www.iit.it/en" target="_blank"><img src="assets/images/istituto-italiano-di-tecnologia-logo.png" alt="IIT Logo" width="15px"></a>
                Â© Last update: 09.2025, Alfredo &nbsp;&nbsp;&nbsp;&nbsp;
                Visitor Number:
            </span>
            <a href='https://www.counter12.com'><img
                    src='https://www.counter12.com/img-49BcxC1Zy9yZbD1C-36.gif'
                    border='0' alt='counter'></a>
                    <script type='text/javascript' src='https://www.counter12.com/ad.js?id=49BcxC1Zy9yZbD1C'></script>
        </div>
    </footer>
</body>

</html>
